{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69081af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torchversion}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torchversion}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install torch-geometric pandas numpy scikit-learn networkx matplotlib seaborn tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klema/miniconda3/envs/graphsage/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.nn import SAGEConv, GATv2Conv, GCNConv\n",
    "import psutil\n",
    "\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    NVML_AVAILABLE = True\n",
    "except:\n",
    "    NVML_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30cfb6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = '/home/klema/sibnn/gnn_tbank/check_notebooks/data/accepted_2007_to_2018Q4.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07403b3a",
   "metadata": {},
   "source": [
    "Lending club dataset (2007-2018)\n",
    "При загрузке данных отбираем только релевантные колонки:\n",
    "1. Числовые: сумма кредита, ставка, доход и др\n",
    "2. Категориальные: Цель кредита, рейтинг, стаж и др\n",
    "\n",
    "Целевая переменная - loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d255901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных\n",
      "Загружено 1345310 узлов, 49 признаков\n",
      "Train/Val/Test: 807186/269062/269062\n",
      "Рёбер: 9756360\n",
      "Распределение классов: Класс 0: 1076751, Класс 1: 268559\n"
     ]
    }
   ],
   "source": [
    "print(\"Загрузка данных\")\n",
    "\n",
    "usecols = [\n",
    "    'loan_amnt', 'int_rate', 'installment', 'grade', 'emp_length', \n",
    "    'home_ownership', 'annual_inc', 'verification_status', 'loan_status',\n",
    "    'purpose', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc',\n",
    "    'pub_rec', 'revol_bal', 'revol_util', 'total_acc'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, usecols=usecols, low_memory=False)\n",
    "\n",
    "# Фильтруем только завершенные займы. Fully Paid - успешно погашен, Charged Off - дефолт.\n",
    "df = df[df['loan_status'].isin(['Fully Paid', 'Charged Off'])]\n",
    "df['target'] = df['loan_status'].map({'Fully Paid': 0, 'Charged Off': 1})\n",
    "\n",
    "numeric_features = ['loan_amnt', 'int_rate', 'installment', 'annual_inc', 'dti',\n",
    "                   'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', \n",
    "                   'revol_bal', 'revol_util', 'total_acc']\n",
    "categorical_features = ['grade', 'emp_length', 'home_ownership', 'verification_status', 'purpose']\n",
    "\n",
    "X_num = df[numeric_features].copy()\n",
    "X_num.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_num.fillna(X_num.median(), inplace=True)\n",
    "\n",
    "X_cat = df[categorical_features].copy()\n",
    "X_cat.fillna('Unknown', inplace=True)\n",
    "X_cat_dummies = pd.get_dummies(X_cat, drop_first=True)\n",
    "\n",
    "X = pd.concat([X_num, X_cat_dummies], axis=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y = df['target'].values\n",
    "\n",
    "# Строим разреженный граф симметричных связей на основе признакового сходства методом k-ближайших соседей.\n",
    "k = 5\n",
    "adj_matrix = kneighbors_graph(X_scaled, k, mode='connectivity', include_self=False)\n",
    "edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "edge_index = to_undirected(edge_index)\n",
    "\n",
    "num_nodes = len(X_scaled)\n",
    "indices = np.arange(num_nodes)\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.4, random_state=42)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "data = Data(\n",
    "    x=torch.tensor(X_scaled, dtype=torch.float),\n",
    "    edge_index=edge_index,\n",
    "    y=torch.tensor(y, dtype=torch.long),\n",
    "    train_mask=train_mask,\n",
    "    val_mask=val_mask,\n",
    "    test_mask=test_mask\n",
    ")\n",
    "\n",
    "print(f\"Загружено {data.x.shape[0]} узлов, {data.x.shape[1]} признаков\")\n",
    "print(f\"Train/Val/Test: {train_mask.sum().item()}/{val_mask.sum().item()}/{test_mask.sum().item()}\")\n",
    "print(f\"Рёбер: {edge_index.shape[1]}\")\n",
    "\n",
    "class_counts = torch.bincount(data.y).tolist()\n",
    "class_dist_str = \", \".join(f\"Класс {i}: {count}\" for i, count in enumerate(class_counts))\n",
    "print(f\"Распределение классов: {class_dist_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5795f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, y):\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    _, out = model(data.x, data.edge_index)\n",
    "    acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n",
    "    return acc\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.sage1 = SAGEConv(dim_in, dim_h)\n",
    "        self.sage2 = SAGEConv(dim_h, dim_out)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                        lr=0.01,\n",
    "                                        weight_decay=5e-4)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.sage1(x, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.sage2(h, edge_index)\n",
    "        return h, F.log_softmax(h, dim=1)\n",
    "\n",
    "    def fit(self, data, epochs):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(epochs+1):\n",
    "            optimizer.zero_grad()\n",
    "            _, out = self(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            acc = accuracy(out[data.train_mask].argmax(dim=1),\n",
    "                          data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "            val_acc = accuracy(out[data.val_mask].argmax(dim=1),\n",
    "                              data.y[data.val_mask])\n",
    "\n",
    "            if(epoch % 10 == 0):\n",
    "                print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc:'\n",
    "                      f' {acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
    "                      f'Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out, heads=4):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads, dropout=0.6)\n",
    "        self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=heads, dropout=0.6)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                        lr=0.005,\n",
    "                                        weight_decay=5e-4)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.dropout(x, p=0.6, training=self.training)\n",
    "        h = self.gat1(h, edge_index)\n",
    "        h = F.elu(h)\n",
    "        h = F.dropout(h, p=0.6, training=self.training)\n",
    "        h = self.gat2(h, edge_index)\n",
    "        return h, F.log_softmax(h, dim=1)\n",
    "\n",
    "    def fit(self, data, epochs):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(epochs+1):\n",
    "            optimizer.zero_grad()\n",
    "            _, out = self(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            acc = accuracy(out[data.train_mask].argmax(dim=1),\n",
    "                          data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "            val_acc = accuracy(out[data.val_mask].argmax(dim=1),\n",
    "                              data.y[data.val_mask])\n",
    "\n",
    "            if(epoch % 10 == 0):\n",
    "                print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc:'\n",
    "                      f' {acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
    "                      f'Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(dim_in, dim_h)\n",
    "        self.gcn2 = GCNConv(dim_h, dim_out)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                        lr=0.01,\n",
    "                                        weight_decay=5e-4)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.dropout(x, p=0.5, training=self.training)\n",
    "        h = self.gcn1(h, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.gcn2(h, edge_index)\n",
    "        return h, F.log_softmax(h, dim=1)\n",
    "\n",
    "    def fit(self, data, epochs):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(epochs+1):\n",
    "            optimizer.zero_grad()\n",
    "            _, out = self(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            acc = accuracy(out[data.train_mask].argmax(dim=1),\n",
    "                          data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "            val_acc = accuracy(out[data.val_mask].argmax(dim=1),\n",
    "                              data.y[data.val_mask])\n",
    "\n",
    "            if(epoch % 10 == 0):\n",
    "                print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc:'\n",
    "                      f' {acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
    "                      f'Val Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "206c6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "\n",
    "def monitor_resources():\n",
    "    stats = {}\n",
    "    # CPU & RAM\n",
    "    stats['ram_mb'] = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    stats['cpu_percent'] = psutil.cpu_percent()\n",
    "\n",
    "    # GPU\n",
    "    if device.type == 'cuda' and NVML_AVAILABLE:\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "        stats['gpu_mem_mb'] = mem_info.used / (1024 ** 2)\n",
    "        stats['gpu_util'] = util.gpu\n",
    "    else:\n",
    "        stats['gpu_mem_mb'] = None\n",
    "        stats['gpu_util'] = None\n",
    "    return stats\n",
    "\n",
    "def train_with_monitoring(model, data, epochs, model_name):\n",
    "    print(f\"\\n{'='*50}\\nTraining {model_name} with resource monitoring\\n{'='*50}\")\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.fit(data, epochs)\n",
    "\n",
    "    final_ram = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    max_gpu_mem = None\n",
    "    if device.type == 'cuda':\n",
    "        max_gpu_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)  # MB\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    test_acc = test(model, data)\n",
    "\n",
    "    results = {\n",
    "        'test_acc': test_acc,\n",
    "        'training_time_sec': duration,\n",
    "        'final_ram_mb': final_ram,\n",
    "        'max_gpu_mem_mb': max_gpu_mem,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{model_name} finished\")\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Training Time: {duration:.1f} sec\")\n",
    "    if max_gpu_mem:\n",
    "        print(f\"Peak GPU Memory: {max_gpu_mem:.1f} MB\")\n",
    "    print(f\"Final RAM Usage: {final_ram:.1f} MB\")\n",
    "\n",
    "    return test_acc, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f408ae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training GraphSAGE with resource monitoring\n",
      "==================================================\n",
      "Epoch   0 | Train Loss: 0.802 | Train Acc:  41.97% | Val Loss: 0.80 | Val Acc: 41.89%\n",
      "Epoch  10 | Train Loss: 0.479 | Train Acc:  78.96% | Val Loss: 0.48 | Val Acc: 79.12%\n",
      "Epoch  20 | Train Loss: 0.467 | Train Acc:  79.97% | Val Loss: 0.46 | Val Acc: 80.14%\n",
      "Epoch  30 | Train Loss: 0.463 | Train Acc:  80.02% | Val Loss: 0.46 | Val Acc: 80.17%\n",
      "Epoch  40 | Train Loss: 0.461 | Train Acc:  80.04% | Val Loss: 0.46 | Val Acc: 80.20%\n",
      "Epoch  50 | Train Loss: 0.459 | Train Acc:  80.09% | Val Loss: 0.46 | Val Acc: 80.24%\n",
      "Epoch  60 | Train Loss: 0.459 | Train Acc:  80.12% | Val Loss: 0.46 | Val Acc: 80.27%\n",
      "Epoch  70 | Train Loss: 0.458 | Train Acc:  80.12% | Val Loss: 0.46 | Val Acc: 80.27%\n",
      "Epoch  80 | Train Loss: 0.458 | Train Acc:  80.14% | Val Loss: 0.46 | Val Acc: 80.29%\n",
      "Epoch  90 | Train Loss: 0.457 | Train Acc:  80.15% | Val Loss: 0.46 | Val Acc: 80.28%\n",
      "Epoch 100 | Train Loss: 0.457 | Train Acc:  80.16% | Val Loss: 0.45 | Val Acc: 80.31%\n",
      "\n",
      "GraphSAGE finished\n",
      "Test Accuracy: 80.18%\n",
      "Training Time: 551.3 sec\n",
      "Final RAM Usage: 8315.4 MB\n",
      "CPU times: user 20min 49s, sys: 12min 55s, total: 33min 45s\n",
      "Wall time: 9min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 1. GraphSAGE\n",
    "graphsage = GraphSAGE(data.x.shape[1], dim_h=64, dim_out=2).to(device)\n",
    "acc_sage, _ = train_with_monitoring(graphsage, data, epochs=100, model_name=\"GraphSAGE\")\n",
    "results['GraphSAGE'] = acc_sage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa11aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training GCN with resource monitoring\n",
      "==================================================\n",
      "Epoch   0 | Train Loss: 1.616 | Train Acc:  25.12% | Val Loss: 1.62 | Val Acc: 25.08%\n",
      "Epoch  10 | Train Loss: 0.595 | Train Acc:  79.38% | Val Loss: 0.59 | Val Acc: 79.58%\n",
      "Epoch  20 | Train Loss: 0.512 | Train Acc:  78.12% | Val Loss: 0.51 | Val Acc: 78.34%\n",
      "Epoch  30 | Train Loss: 0.486 | Train Acc:  79.23% | Val Loss: 0.48 | Val Acc: 79.43%\n",
      "Epoch  40 | Train Loss: 0.477 | Train Acc:  79.63% | Val Loss: 0.47 | Val Acc: 79.82%\n",
      "Epoch  50 | Train Loss: 0.471 | Train Acc:  79.77% | Val Loss: 0.47 | Val Acc: 79.96%\n",
      "Epoch  60 | Train Loss: 0.468 | Train Acc:  79.87% | Val Loss: 0.47 | Val Acc: 80.03%\n",
      "Epoch  70 | Train Loss: 0.467 | Train Acc:  79.96% | Val Loss: 0.46 | Val Acc: 80.11%\n",
      "Epoch  80 | Train Loss: 0.466 | Train Acc:  80.00% | Val Loss: 0.46 | Val Acc: 80.15%\n",
      "Epoch  90 | Train Loss: 0.465 | Train Acc:  80.02% | Val Loss: 0.46 | Val Acc: 80.19%\n",
      "Epoch 100 | Train Loss: 0.464 | Train Acc:  80.03% | Val Loss: 0.46 | Val Acc: 80.17%\n",
      "\n",
      "GCN finished\n",
      "Test Accuracy: 80.11%\n",
      "Training Time: 639.2 sec\n",
      "Final RAM Usage: 7862.3 MB\n",
      "CPU times: user 17min 4s, sys: 17min 33s, total: 34min 38s\n",
      "Wall time: 10min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 2. GCN\n",
    "gcn = GCN(data.x.shape[1], dim_h=64, dim_out=2).to(device)\n",
    "acc_gcn, _ = train_with_monitoring(gcn, data, epochs=100, model_name=\"GCN\")\n",
    "results['GCN'] = acc_gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91553702",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 3. GAT\n",
    "gat = GAT(data.x.shape[1], dim_h=32, dim_out=2, heads=4).to(device)\n",
    "acc_gat, _ = train_with_monitoring(gat, data, epochs=100, model_name=\"GAT\")\n",
    "results['GAT'] = acc_gat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776fa43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
