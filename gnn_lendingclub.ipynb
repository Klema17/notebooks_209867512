{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69081af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torchversion}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torchversion}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install torch-geometric pandas numpy scikit-learn networkx matplotlib seaborn tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa51c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klema/miniconda3/envs/graphsage/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv, SAGEConv\n",
    "from torch_geometric.utils import to_undirected\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    NVML_AVAILABLE = True\n",
    "except:\n",
    "    NVML_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30cfb6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = '/home/klema/sibnn/gnn_tbank/check_notebooks/data/accepted_2007_to_2018Q4.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07403b3a",
   "metadata": {},
   "source": [
    "Lending club dataset (2007-2018)\n",
    "При загрузке данных отбираем только релевантные колонки:\n",
    "1. Числовые: сумма кредита, ставка, доход и др\n",
    "2. Категориальные: Цель кредита, рейтинг, стаж и др\n",
    "\n",
    "Целевая переменная - loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d255901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных\n",
      "Загружено 1345310 узлов, 49 признаков\n",
      "Train/Val/Test: 807186/269062/269062\n",
      "Рёбер: 9756360\n",
      "Распределение классов: Класс 0: 1076751, Класс 1: 268559\n"
     ]
    }
   ],
   "source": [
    "print(\"Загрузка данных\")\n",
    "\n",
    "usecols = [\n",
    "    'loan_amnt', 'int_rate', 'installment', 'grade', 'emp_length', \n",
    "    'home_ownership', 'annual_inc', 'verification_status', 'loan_status',\n",
    "    'purpose', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc',\n",
    "    'pub_rec', 'revol_bal', 'revol_util', 'total_acc'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, usecols=usecols, low_memory=False)\n",
    "\n",
    "# Фильтруем только завершенные займы. Fully Paid - успешно погашен, Charged Off - дефолт.\n",
    "df = df[df['loan_status'].isin(['Fully Paid', 'Charged Off'])]\n",
    "df['target'] = df['loan_status'].map({'Fully Paid': 0, 'Charged Off': 1})\n",
    "\n",
    "numeric_features = ['loan_amnt', 'int_rate', 'installment', 'annual_inc', 'dti',\n",
    "                   'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', \n",
    "                   'revol_bal', 'revol_util', 'total_acc']\n",
    "categorical_features = ['grade', 'emp_length', 'home_ownership', 'verification_status', 'purpose']\n",
    "\n",
    "X_num = df[numeric_features].copy()\n",
    "X_num.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_num.fillna(X_num.median(), inplace=True)\n",
    "\n",
    "X_cat = df[categorical_features].copy()\n",
    "X_cat.fillna('Unknown', inplace=True)\n",
    "X_cat_dummies = pd.get_dummies(X_cat, drop_first=True)\n",
    "\n",
    "X = pd.concat([X_num, X_cat_dummies], axis=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y = df['target'].values\n",
    "\n",
    "# Строим разреженный граф симметричных связей на основе признакового сходства методом k-ближайших соседей.\n",
    "k = 5\n",
    "adj_matrix = kneighbors_graph(X_scaled, k, mode='connectivity', include_self=False)\n",
    "edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "edge_index = to_undirected(edge_index)\n",
    "\n",
    "num_nodes = len(X_scaled)\n",
    "indices = np.arange(num_nodes)\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.4, random_state=42)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "data = Data(\n",
    "    x=torch.tensor(X_scaled, dtype=torch.float),\n",
    "    edge_index=edge_index,\n",
    "    y=torch.tensor(y, dtype=torch.long),\n",
    "    train_mask=train_mask,\n",
    "    val_mask=val_mask,\n",
    "    test_mask=test_mask\n",
    ")\n",
    "\n",
    "print(f\"Загружено {data.x.shape[0]} узлов, {data.x.shape[1]} признаков\")\n",
    "print(f\"Train/Val/Test: {train_mask.sum().item()}/{val_mask.sum().item()}/{test_mask.sum().item()}\")\n",
    "print(f\"Рёбер: {edge_index.shape[1]}\")\n",
    "\n",
    "class_counts = torch.bincount(data.y).tolist()\n",
    "class_dist_str = \", \".join(f\"Класс {i}: {count}\" for i, count in enumerate(class_counts))\n",
    "print(f\"Распределение классов: {class_dist_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db268092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление весов перед обучением\n",
    "y_train = data.y[data.train_mask].cpu().numpy()\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(data.x.device)\n",
    "\n",
    "# Использование в лоссе\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc1f1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred, true, probs=None):\n",
    "    pred = pred.cpu().numpy()\n",
    "    true = true.cpu().numpy()\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (pred == true).mean(),\n",
    "        'precision': precision_score(true, pred, zero_division=0),\n",
    "        'recall': recall_score(true, pred, zero_division=0),\n",
    "        'f1': f1_score(true, pred, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    if probs is not None:\n",
    "        probs = probs.cpu().numpy()\n",
    "        metrics['roc_auc'] = roc_auc_score(true, probs)\n",
    "        metrics['pr_auc'] = average_precision_score(true, probs)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "def test(model, data):\n",
    "    \"\"\"Тестирование с полным набором метрик\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)[data.test_mask]\n",
    "        probs = torch.exp(out)[:, 1][data.test_mask]  # Вероятность класса 1 (дефолт)\n",
    "        true = data.y[data.test_mask]\n",
    "        \n",
    "        return compute_metrics(pred, true, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5795f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.sage1 = SAGEConv(dim_in, dim_h)\n",
    "        self.sage2 = SAGEConv(dim_h, dim_out)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                        lr=0.01,\n",
    "                                        weight_decay=5e-4)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.sage1(x, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.sage2(h, edge_index)\n",
    "        return h, F.log_softmax(h, dim=1)\n",
    "\n",
    "    def fit(self, data, epochs):\n",
    "        # Взвешенный лосс\n",
    "        y_train = data.y[data.train_mask].cpu().numpy()\n",
    "        weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        weights = torch.tensor(weights, dtype=torch.float).to(data.x.device)\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            _, out = self(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                train_pred = out[data.train_mask].argmax(dim=1)\n",
    "                train_metrics = compute_metrics(train_pred, data.y[data.train_mask])\n",
    "                \n",
    "                val_pred = out[data.val_mask].argmax(dim=1)\n",
    "                val_probs = torch.exp(out)[:, 1][data.val_mask]\n",
    "                val_metrics = compute_metrics(val_pred, data.y[data.val_mask], probs=val_probs)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch:>3} | '\n",
    "                    f'TL: {loss:.3f} | TF1: {train_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VL: {val_loss:.3f} | VF1: {val_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VRec: {val_metrics[\"recall\"]:.3f} | VPR: {val_metrics[\"pr_auc\"]:.3f}')\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out, heads=4):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads, concat=True, dropout=0.6)\n",
    "        self.gat2 = GATv2Conv(dim_h * heads, dim_out, heads=heads, concat=False, dropout=0.6)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                        lr=0.005,\n",
    "                                        weight_decay=5e-4)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.dropout(x, p=0.6, training=self.training)\n",
    "        h = self.gat1(h, edge_index)\n",
    "        h = F.elu(h)\n",
    "        h = F.dropout(h, p=0.6, training=self.training)\n",
    "        h = self.gat2(h, edge_index)\n",
    "        return h, F.log_softmax(h, dim=1)\n",
    "\n",
    "    def fit(self, data, epochs):\n",
    "        # Взвешенный лосс\n",
    "        y_train = data.y[data.train_mask].cpu().numpy()\n",
    "        weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        weights = torch.tensor(weights, dtype=torch.float).to(data.x.device)\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            _, out = self(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                train_pred = out[data.train_mask].argmax(dim=1)\n",
    "                train_metrics = compute_metrics(train_pred, data.y[data.train_mask])\n",
    "                \n",
    "                val_pred = out[data.val_mask].argmax(dim=1)\n",
    "                val_probs = torch.exp(out)[:, 1][data.val_mask]\n",
    "                val_metrics = compute_metrics(val_pred, data.y[data.val_mask], probs=val_probs)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch:>3} | '\n",
    "                    f'TL: {loss:.3f} | TF1: {train_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VL: {val_loss:.3f} | VF1: {val_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VRec: {val_metrics[\"recall\"]:.3f} | VPR: {val_metrics[\"pr_auc\"]:.3f}')\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(dim_in, dim_h)\n",
    "        self.gcn2 = GCNConv(dim_h, dim_out)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                        lr=0.01,\n",
    "                                        weight_decay=5e-4)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.dropout(x, p=0.5, training=self.training)\n",
    "        h = self.gcn1(h, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.gcn2(h, edge_index)\n",
    "        return h, F.log_softmax(h, dim=1)\n",
    "\n",
    "    def fit(self, data, epochs):\n",
    "        # Взвешенный лосс\n",
    "        y_train = data.y[data.train_mask].cpu().numpy()\n",
    "        weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        weights = torch.tensor(weights, dtype=torch.float).to(data.x.device)\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            _, out = self(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                train_pred = out[data.train_mask].argmax(dim=1)\n",
    "                train_metrics = compute_metrics(train_pred, data.y[data.train_mask])\n",
    "                \n",
    "                val_pred = out[data.val_mask].argmax(dim=1)\n",
    "                val_probs = torch.exp(out)[:, 1][data.val_mask]\n",
    "                val_metrics = compute_metrics(val_pred, data.y[data.val_mask], probs=val_probs)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch:>3} | '\n",
    "                    f'TL: {loss:.3f} | TF1: {train_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VL: {val_loss:.3f} | VF1: {val_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VRec: {val_metrics[\"recall\"]:.3f} | VPR: {val_metrics[\"pr_auc\"]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "206c6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "\n",
    "def monitor_resources():\n",
    "    stats = {}\n",
    "    # CPU & RAM\n",
    "    stats['ram_mb'] = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    stats['cpu_percent'] = psutil.cpu_percent()\n",
    "\n",
    "    # GPU\n",
    "    if device.type == 'cuda' and NVML_AVAILABLE:\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "        stats['gpu_mem_mb'] = mem_info.used / (1024 ** 2)\n",
    "        stats['gpu_util'] = util.gpu\n",
    "    else:\n",
    "        stats['gpu_mem_mb'] = None\n",
    "        stats['gpu_util'] = None\n",
    "    return stats\n",
    "\n",
    "def train_with_monitoring(model, data, epochs, model_name):\n",
    "    print(f\"\\n{'='*50}\\nTraining {model_name} with resource monitoring\\n{'='*50}\")\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.fit(data, epochs)\n",
    "\n",
    "    final_ram = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    max_gpu_mem = None\n",
    "    if device.type == 'cuda':\n",
    "        max_gpu_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    test_metrics = test(model, data)\n",
    "\n",
    "    results = {\n",
    "        'test_metrics': test_metrics,\n",
    "        'training_time_sec': duration,\n",
    "        'final_ram_mb': final_ram,\n",
    "        'max_gpu_mem_mb': max_gpu_mem,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{model_name} finished\")\n",
    "    print(f\"Test Recall: {test_metrics.get('recall', 0):.3f} | \"\n",
    "          f\"F1: {test_metrics.get('f1', 0):.3f} | \"\n",
    "          f\"PR-AUC: {test_metrics.get('pr_auc', 0):.3f}\")\n",
    "    print(f\"Training Time: {duration:.1f} sec\")\n",
    "    if max_gpu_mem:\n",
    "        print(f\"Peak GPU Memory: {max_gpu_mem:.1f} MB\")\n",
    "    print(f\"Final RAM Usage: {final_ram:.1f} MB\")\n",
    "\n",
    "    return test_metrics.get('recall', 0), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f408ae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training GraphSAGE with resource monitoring\n",
      "==================================================\n",
      "Epoch   0 | TL: 0.783 | TF1: 0.219 | VL: 0.782 | VF1: 0.219 | VRec: 0.273 | VPR: 0.187\n",
      "Epoch  10 | TL: 0.645 | TF1: 0.408 | VL: 0.644 | VF1: 0.406 | VRec: 0.732 | VPR: 0.329\n",
      "Epoch  20 | TL: 0.631 | TF1: 0.420 | VL: 0.630 | VF1: 0.419 | VRec: 0.678 | VPR: 0.352\n",
      "Epoch  30 | TL: 0.628 | TF1: 0.421 | VL: 0.627 | VF1: 0.421 | VRec: 0.689 | VPR: 0.357\n",
      "Epoch  40 | TL: 0.627 | TF1: 0.423 | VL: 0.626 | VF1: 0.422 | VRec: 0.686 | VPR: 0.359\n",
      "Epoch  50 | TL: 0.625 | TF1: 0.424 | VL: 0.624 | VF1: 0.423 | VRec: 0.679 | VPR: 0.362\n",
      "Epoch  60 | TL: 0.625 | TF1: 0.425 | VL: 0.624 | VF1: 0.423 | VRec: 0.676 | VPR: 0.363\n",
      "Epoch  70 | TL: 0.624 | TF1: 0.426 | VL: 0.623 | VF1: 0.425 | VRec: 0.682 | VPR: 0.366\n",
      "Epoch  80 | TL: 0.623 | TF1: 0.426 | VL: 0.623 | VF1: 0.426 | VRec: 0.681 | VPR: 0.366\n",
      "Epoch  90 | TL: 0.623 | TF1: 0.427 | VL: 0.622 | VF1: 0.426 | VRec: 0.680 | VPR: 0.368\n",
      "Epoch 100 | TL: 0.622 | TF1: 0.427 | VL: 0.622 | VF1: 0.426 | VRec: 0.681 | VPR: 0.369\n",
      "\n",
      "GraphSAGE finished\n",
      "Test Recall: 0.681 | F1: 0.429 | PR-AUC: 0.377\n",
      "Training Time: 882.8 sec\n",
      "Final RAM Usage: 9795.9 MB\n",
      "CPU times: user 32min 12s, sys: 23min 8s, total: 55min 20s\n",
      "Wall time: 14min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 1. GraphSAGE\n",
    "graphsage = GraphSAGE(data.x.shape[1], dim_h=64, dim_out=2).to(device)\n",
    "acc_sage, _ = train_with_monitoring(graphsage, data, epochs=100, model_name=\"GraphSAGE\")\n",
    "results['GraphSAGE'] = acc_sage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efa11aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training GCN with resource monitoring\n",
      "==================================================\n",
      "Epoch   0 | TL: 0.919 | TF1: 0.212 | VL: 0.921 | VF1: 0.212 | VRec: 0.297 | VPR: 0.173\n",
      "Epoch  10 | TL: 0.666 | TF1: 0.405 | VL: 0.665 | VF1: 0.403 | VRec: 0.599 | VPR: 0.327\n",
      "Epoch  20 | TL: 0.645 | TF1: 0.404 | VL: 0.644 | VF1: 0.403 | VRec: 0.624 | VPR: 0.328\n",
      "Epoch  30 | TL: 0.636 | TF1: 0.415 | VL: 0.635 | VF1: 0.414 | VRec: 0.657 | VPR: 0.339\n",
      "Epoch  40 | TL: 0.634 | TF1: 0.416 | VL: 0.633 | VF1: 0.415 | VRec: 0.681 | VPR: 0.343\n",
      "Epoch  50 | TL: 0.632 | TF1: 0.417 | VL: 0.632 | VF1: 0.416 | VRec: 0.680 | VPR: 0.346\n",
      "Epoch  60 | TL: 0.631 | TF1: 0.418 | VL: 0.631 | VF1: 0.417 | VRec: 0.679 | VPR: 0.347\n",
      "Epoch  70 | TL: 0.631 | TF1: 0.419 | VL: 0.630 | VF1: 0.417 | VRec: 0.679 | VPR: 0.349\n",
      "Epoch  80 | TL: 0.630 | TF1: 0.419 | VL: 0.630 | VF1: 0.418 | VRec: 0.681 | VPR: 0.350\n",
      "Epoch  90 | TL: 0.630 | TF1: 0.420 | VL: 0.630 | VF1: 0.418 | VRec: 0.682 | VPR: 0.351\n",
      "Epoch 100 | TL: 0.630 | TF1: 0.420 | VL: 0.629 | VF1: 0.419 | VRec: 0.685 | VPR: 0.351\n",
      "\n",
      "GCN finished\n",
      "Test Recall: 0.696 | F1: 0.422 | PR-AUC: 0.361\n",
      "Training Time: 1057.5 sec\n",
      "Final RAM Usage: 7271.8 MB\n",
      "CPU times: user 27min 21s, sys: 31min 54s, total: 59min 15s\n",
      "Wall time: 17min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 2. GCN\n",
    "gcn = GCN(data.x.shape[1], dim_h=64, dim_out=2).to(device)\n",
    "acc_gcn, _ = train_with_monitoring(gcn, data, epochs=100, model_name=\"GCN\")\n",
    "results['GCN'] = acc_gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91553702",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 3. GAT\n",
    "gat = GAT(data.x.shape[1], dim_h=32, dim_out=2, heads=4).to(device)\n",
    "acc_gat, _ = train_with_monitoring(gat, data, epochs=100, model_name=\"GAT\")\n",
    "results['GAT'] = acc_gat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776fa43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
