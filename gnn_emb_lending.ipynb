{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bf8c4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klema/miniconda3/envs/graphsage/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv, SAGEConv\n",
    "from torch_geometric.utils import to_undirected\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    NVML_AVAILABLE = True\n",
    "except:\n",
    "    NVML_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71db9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = '/home/klema/sibnn/gnn_tbank/check_notebooks/data/accepted_2007_to_2018Q4.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных (эмбеддинги для категориальных признаков, граф — только на числовых)\n",
      "  grade: 7 уникальных значений\n",
      "  emp_length: 12 уникальных значений\n",
      "  home_ownership: 6 уникальных значений\n",
      "  verification_status: 3 уникальных значений\n",
      "  purpose: 14 уникальных значений\n"
     ]
    }
   ],
   "source": [
    "print(\"Загрузка данных (эмбеддинги для категориальных признаков, граф — только на числовых)\")\n",
    "\n",
    "usecols = [\n",
    "    'loan_amnt', 'int_rate', 'installment', 'grade', 'emp_length', \n",
    "    'home_ownership', 'annual_inc', 'verification_status', 'loan_status',\n",
    "    'purpose', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc',\n",
    "    'pub_rec', 'revol_bal', 'revol_util', 'total_acc'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, usecols=usecols, low_memory=False)\n",
    "\n",
    "# Фильтрация завершённых займов\n",
    "df = df[df['loan_status'].isin(['Fully Paid', 'Charged Off'])]\n",
    "y = df['loan_status'].map({'Fully Paid': 0, 'Charged Off': 1}).values\n",
    "\n",
    "# Числовые признаки\n",
    "numeric_features = ['loan_amnt', 'int_rate', 'installment', 'annual_inc', 'dti',\n",
    "                   'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', \n",
    "                   'revol_bal', 'revol_util', 'total_acc']\n",
    "X_num = df[numeric_features].copy()\n",
    "X_num.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_num.fillna(X_num.median(), inplace=True)\n",
    "\n",
    "# Стандартизация числовых признаков (ТОЛЬКО на них строим граф!)\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)\n",
    "\n",
    "# Категориальные признаки -> индексы для эмбеддингов\n",
    "categorical_features = ['grade', 'emp_length', 'home_ownership', 'verification_status', 'purpose']\n",
    "X_cat = df[categorical_features].copy()\n",
    "X_cat.fillna('Unknown', inplace=True)\n",
    "\n",
    "# Label Encoding для каждого категориального признака\n",
    "X_cat_encoded = np.zeros((len(df), len(categorical_features)), dtype=np.int64)\n",
    "cat_dims = []\n",
    "\n",
    "for i, col in enumerate(categorical_features):\n",
    "    le = LabelEncoder()\n",
    "    X_cat_encoded[:, i] = le.fit_transform(X_cat[col].astype(str))\n",
    "    cat_dims.append(len(le.classes_))\n",
    "    print(f\"  {col}: {len(le.classes_)} уникальных значений\")\n",
    "\n",
    "k = 10\n",
    "adj_matrix = kneighbors_graph(X_num_scaled, k, mode='connectivity', include_self=False)\n",
    "edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "edge_index = to_undirected(edge_index)\n",
    "\n",
    "# Разделение на train/val/test с сохранением баланса классов\n",
    "num_nodes = len(df)\n",
    "indices = np.arange(num_nodes)\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.4, random_state=42, stratify=y)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42, stratify=y[temp_idx])\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "# Создание объекта данных с разделенными признаками\n",
    "data = Data(\n",
    "    x_num=torch.tensor(X_num_scaled, dtype=torch.float),\n",
    "    x_cat=torch.tensor(X_cat_encoded, dtype=torch.long),\n",
    "    edge_index=edge_index,\n",
    "    y=torch.tensor(y, dtype=torch.long),\n",
    "    train_mask=train_mask,\n",
    "    val_mask=val_mask,\n",
    "    test_mask=test_mask\n",
    ")\n",
    "\n",
    "# Метаданные для моделей\n",
    "data.num_numerical = X_num_scaled.shape[1]\n",
    "data.num_categorical = len(categorical_features)\n",
    "data.cat_dims = cat_dims\n",
    "\n",
    "print(f\"\\nЗагружено {num_nodes} узлов\")\n",
    "print(f\"Числовые признаки: {data.num_numerical}\")\n",
    "print(f\"Категориальные признаки: {data.num_categorical} ({categorical_features})\")\n",
    "print(f\"Уникальные значения по кат. признакам: {cat_dims}\")\n",
    "print(f\"Train/Val/Test: {train_mask.sum().item()}/{val_mask.sum().item()}/{test_mask.sum().item()}\")\n",
    "print(f\"Рёбер: {edge_index.shape[1]} (k={k})\")\n",
    "\n",
    "class_counts = np.bincount(y)\n",
    "print(f\"Распределение классов: Fully Paid (0): {class_counts[0]}, Charged Off (1): {class_counts[1]}\")\n",
    "print(f\"Доля дефолтов: {class_counts[1] / num_nodes:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ddc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление весов перед обучением\n",
    "y_train = data.y[data.train_mask].cpu().numpy()\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Используем устройство целевой переменной (всегда присутствует)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(data.y.device)\n",
    "\n",
    "# Использование в лоссе\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred, true, probs=None):\n",
    "    pred = pred.cpu().numpy()\n",
    "    true = true.cpu().numpy()\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (pred == true).mean(),\n",
    "        'precision': precision_score(true, pred, zero_division=0),\n",
    "        'recall': recall_score(true, pred, zero_division=0),\n",
    "        'f1': f1_score(true, pred, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    if probs is not None:\n",
    "        probs = probs.cpu().numpy()\n",
    "        metrics['roc_auc'] = roc_auc_score(true, probs)\n",
    "        metrics['pr_auc'] = average_precision_score(true, probs)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "def test(model, data):\n",
    "    \"\"\"Тестирование с поддержкой обеих сигнатур: (x, edge_index) и (x_num, x_cat, edge_index)\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Автоматическое определение сигнатуры модели по количеству параметров в forward\n",
    "        if hasattr(data, 'x_num') and hasattr(data, 'x_cat'):\n",
    "            _, out = model(data.x_num, data.x_cat, data.edge_index)\n",
    "        else:\n",
    "            _, out = model(data.x, data.edge_index)\n",
    "        \n",
    "        pred = out.argmax(dim=1)[data.test_mask]\n",
    "        probs = torch.exp(out)[:, 1][data.test_mask]  # Вероятность класса 1 (дефолт)\n",
    "        true = data.y[data.test_mask]\n",
    "        \n",
    "        return compute_metrics(pred, true, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeatureEncoder(torch.nn.Module):\n",
    "    def __init__(self, cat_dims, cat_emb_dims, num_numerical, output_dim):\n",
    "        super().__init__()\n",
    "        assert len(cat_dims) == len(cat_emb_dims)\n",
    "        \n",
    "        self.embeddings = torch.nn.ModuleList([\n",
    "            torch.nn.Embedding(num_categories, emb_dim)\n",
    "            for num_categories, emb_dim in zip(cat_dims, cat_emb_dims)\n",
    "        ])\n",
    "        \n",
    "        total_input_dim = sum(cat_emb_dims) + num_numerical\n",
    "        self.projector = torch.nn.Sequential(\n",
    "            torch.nn.Linear(total_input_dim, output_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_num, x_cat):\n",
    "        embeddings = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        x = torch.cat([x_num] + embeddings, dim=1)\n",
    "        return self.projector(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c31db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, cat_dims, cat_emb_dims, num_numerical, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        # Энкодер признаков: эмбеддинги + проекция\n",
    "        self.feature_encoder = FeatureEncoder(\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dims=cat_emb_dims,\n",
    "            num_numerical=num_numerical,\n",
    "            output_dim=dim_h # проецируем сразу в размерность первого GCN-слоя\n",
    "        )\n",
    "        self.sage1 = SAGEConv(dim_h, dim_h)\n",
    "        self.sage2 = SAGEConv(dim_h, dim_out)\n",
    "        \n",
    "        # Оптимизатор с дифференцированной регуляризацией\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.feature_encoder.embeddings.parameters(), 'weight_decay': 1e-3},\n",
    "            {'params': self.feature_encoder.projector.parameters(), 'weight_decay': 5e-4},\n",
    "            {'params': self.gcn1.parameters(), 'weight_decay': 5e-4},\n",
    "            {'params': self.gcn2.parameters(), 'weight_decay': 5e-4},\n",
    "        ], lr=0.02)\n",
    "\n",
    "    def forward(self, x_num, x_cat, edge_index):\n",
    "        x = self.feature_encoder(x_num, x_cat)\n",
    "        h = self.sage1(x, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.sage2(h, edge_index)\n",
    "        return h, F.log_softmax(h, dim=1)\n",
    "\n",
    "    def fit(self, data, epochs, patience=20):\n",
    "        # Взвешенный лосс для дисбаланса классов\n",
    "        y_train = data.y[data.train_mask].cpu().numpy()\n",
    "        weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        weights = torch.tensor(weights, dtype=torch.float).to(data.x_num.device)\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            _, out = self(data.x_num, data.x_cat, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                train_pred = out[data.train_mask].argmax(dim=1)\n",
    "                train_metrics = compute_metrics(train_pred, data.y[data.train_mask])\n",
    "                \n",
    "                val_pred = out[data.val_mask].argmax(dim=1)\n",
    "                val_probs = torch.exp(out)[:, 1][data.val_mask]\n",
    "                val_metrics = compute_metrics(val_pred, data.y[data.val_mask], probs=val_probs)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            # Ранняя остановка\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_state = {k: v.cpu() for k, v in self.state_dict().items()}\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience and epoch > 30:\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch} (best val loss: {best_val_loss:.3f})\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    break\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch:>3} | '\n",
    "                    f'TL: {loss:.3f} | TF1: {train_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VL: {val_loss:.3f} | VF1: {val_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VRec: {val_metrics[\"recall\"]:.3f} | VPR: {val_metrics[\"pr_auc\"]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e446e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, cat_dims, cat_emb_dims, num_numerical, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        # Энкодер признаков: эмбеддинги + проекция\n",
    "        self.feature_encoder = FeatureEncoder(\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dims=cat_emb_dims,\n",
    "            num_numerical=num_numerical,\n",
    "            output_dim=dim_h  # проецируем сразу в размерность первого GCN-слоя\n",
    "        )\n",
    "        \n",
    "        self.gcn1 = GCNConv(dim_h, dim_h)\n",
    "        self.gcn2 = GCNConv(dim_h, dim_out)\n",
    "        \n",
    "        # Оптимизатор с дифференцированной регуляризацией\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.feature_encoder.embeddings.parameters(), 'weight_decay': 1e-3},\n",
    "            {'params': self.feature_encoder.projector.parameters(), 'weight_decay': 5e-4},\n",
    "            {'params': self.gcn1.parameters(), 'weight_decay': 5e-4},\n",
    "            {'params': self.gcn2.parameters(), 'weight_decay': 5e-4},\n",
    "        ], lr=0.02)\n",
    "\n",
    "    def forward(self, x_num, x_cat, edge_index):\n",
    "        # Кодирование признаков через эмбеддинги\n",
    "        x = self.feature_encoder(x_num, x_cat)\n",
    "        \n",
    "        h = F.dropout(x, p=0.3, training=self.training)\n",
    "        h = self.gcn1(h, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.gcn2(h, edge_index)\n",
    "        return h, F.log_softmax(h, dim=1)\n",
    "\n",
    "    def fit(self, data, epochs, patience=20):\n",
    "        # Взвешенный лосс для дисбаланса классов\n",
    "        y_train = data.y[data.train_mask].cpu().numpy()\n",
    "        weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        weights = torch.tensor(weights, dtype=torch.float).to(data.x_num.device)\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            _, out = self(data.x_num, data.x_cat, data.edge_index)  # ← ключевое изменение\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Метрики на трейне\n",
    "                train_pred = out[data.train_mask].argmax(dim=1)\n",
    "                train_metrics = compute_metrics(train_pred, data.y[data.train_mask])\n",
    "                \n",
    "                # Метрики на валидации\n",
    "                val_pred = out[data.val_mask].argmax(dim=1)\n",
    "                val_probs = torch.exp(out)[:, 1][data.val_mask]\n",
    "                val_metrics = compute_metrics(val_pred, data.y[data.val_mask], probs=val_probs)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            # Ранняя остановка\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_state = {k: v.cpu() for k, v in self.state_dict().items()}\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience and epoch > 30:\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch} (best val loss: {best_val_loss:.3f})\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    break\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch:>3} | '\n",
    "                    f'TL: {loss:.3f} | TF1: {train_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VL: {val_loss:.3f} | VF1: {val_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VRec: {val_metrics[\"recall\"]:.3f} | VPR: {val_metrics[\"pr_auc\"]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "\n",
    "def monitor_resources():\n",
    "    stats = {}\n",
    "    # CPU & RAM\n",
    "    stats['ram_mb'] = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    stats['cpu_percent'] = psutil.cpu_percent()\n",
    "\n",
    "    # GPU\n",
    "    if device.type == 'cuda' and NVML_AVAILABLE:\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "        stats['gpu_mem_mb'] = mem_info.used / (1024 ** 2)\n",
    "        stats['gpu_util'] = util.gpu\n",
    "    else:\n",
    "        stats['gpu_mem_mb'] = None\n",
    "        stats['gpu_util'] = None\n",
    "    return stats\n",
    "\n",
    "def train_with_monitoring(model, data, epochs, model_name):\n",
    "    print(f\"\\nTraining {model_name} with resource monitoring\\n\")\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.fit(data, epochs)\n",
    "\n",
    "    final_ram = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    max_gpu_mem = None\n",
    "    if device.type == 'cuda':\n",
    "        max_gpu_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    test_metrics = test(model, data)\n",
    "\n",
    "    results = {\n",
    "        'test_metrics': test_metrics,\n",
    "        'training_time_sec': duration,\n",
    "        'final_ram_mb': final_ram,\n",
    "        'max_gpu_mem_mb': max_gpu_mem,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{model_name} finished\")\n",
    "    print(f\"Test Recall: {test_metrics.get('recall', 0):.3f} | \"\n",
    "          f\"F1: {test_metrics.get('f1', 0):.3f} | \"\n",
    "          f\"PR-AUC: {test_metrics.get('pr_auc', 0):.3f}\")\n",
    "    print(f\"Training Time: {duration:.1f} sec\")\n",
    "    if max_gpu_mem:\n",
    "        print(f\"Peak GPU Memory: {max_gpu_mem:.1f} MB\")\n",
    "    print(f\"Final RAM Usage: {final_ram:.1f} MB\")\n",
    "\n",
    "    return test_metrics.get('recall', 0), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebffd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерности эмбеддингов: [4, 5, 4, 4, 5]\n",
      "Суммарная размерность после конкатенации: 34\n",
      "\n",
      "==================================================\n",
      "Training GraphSAGE (с эмбеддингами) with resource monitoring\n",
      "==================================================\n",
      "Epoch   0 | TL: 0.696 | TF1: 0.303 | VL: 0.696 | VF1: 0.303 | VRec: 0.603 | VPR: 0.206\n",
      "Epoch  10 | TL: 0.674 | TF1: 0.393 | VL: 0.675 | VF1: 0.392 | VRec: 0.529 | VPR: 0.326\n",
      "Epoch  20 | TL: 0.641 | TF1: 0.414 | VL: 0.643 | VF1: 0.412 | VRec: 0.667 | VPR: 0.340\n",
      "Epoch  30 | TL: 0.635 | TF1: 0.412 | VL: 0.636 | VF1: 0.410 | VRec: 0.734 | VPR: 0.344\n",
      "Epoch  40 | TL: 0.632 | TF1: 0.415 | VL: 0.633 | VF1: 0.414 | VRec: 0.689 | VPR: 0.348\n",
      "Epoch  50 | TL: 0.631 | TF1: 0.417 | VL: 0.632 | VF1: 0.415 | VRec: 0.681 | VPR: 0.350\n",
      "Epoch  60 | TL: 0.630 | TF1: 0.418 | VL: 0.630 | VF1: 0.417 | VRec: 0.688 | VPR: 0.352\n",
      "Epoch  70 | TL: 0.628 | TF1: 0.420 | VL: 0.628 | VF1: 0.419 | VRec: 0.685 | VPR: 0.355\n",
      "Epoch  80 | TL: 0.626 | TF1: 0.422 | VL: 0.627 | VF1: 0.420 | VRec: 0.671 | VPR: 0.358\n",
      "Epoch  90 | TL: 0.628 | TF1: 0.423 | VL: 0.628 | VF1: 0.422 | VRec: 0.627 | VPR: 0.360\n",
      "Epoch 100 | TL: 0.626 | TF1: 0.424 | VL: 0.627 | VF1: 0.423 | VRec: 0.653 | VPR: 0.360\n",
      "Epoch 110 | TL: 0.625 | TF1: 0.425 | VL: 0.625 | VF1: 0.424 | VRec: 0.648 | VPR: 0.362\n",
      "Epoch 120 | TL: 0.624 | TF1: 0.425 | VL: 0.624 | VF1: 0.424 | VRec: 0.686 | VPR: 0.364\n",
      "Epoch 130 | TL: 0.623 | TF1: 0.426 | VL: 0.624 | VF1: 0.425 | VRec: 0.668 | VPR: 0.365\n",
      "Epoch 140 | TL: 0.623 | TF1: 0.426 | VL: 0.623 | VF1: 0.425 | VRec: 0.649 | VPR: 0.367\n",
      "Epoch 150 | TL: 0.623 | TF1: 0.426 | VL: 0.623 | VF1: 0.425 | VRec: 0.682 | VPR: 0.366\n",
      "Epoch 160 | TL: 0.622 | TF1: 0.427 | VL: 0.623 | VF1: 0.426 | VRec: 0.666 | VPR: 0.368\n",
      "Epoch 170 | TL: 0.622 | TF1: 0.426 | VL: 0.623 | VF1: 0.425 | VRec: 0.688 | VPR: 0.368\n",
      "Epoch 180 | TL: 0.622 | TF1: 0.425 | VL: 0.623 | VF1: 0.424 | VRec: 0.705 | VPR: 0.367\n",
      "Epoch 190 | TL: 0.623 | TF1: 0.428 | VL: 0.623 | VF1: 0.428 | VRec: 0.639 | VPR: 0.368\n",
      "\n",
      "⏹️  Early stopping at epoch 193 (best val loss: 0.622)\n",
      "\n",
      "GraphSAGE (с эмбеддингами) finished\n",
      "Test Recall: 0.664 | F1: 0.429 | PR-AUC: 0.373\n",
      "Training Time: 2294.2 sec\n",
      "Final RAM Usage: 6458.9 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "\n",
    "# Адаптивные размерности эмбеддингов: √(уникальных значений) + небольшой запас\n",
    "cat_emb_dims = [\n",
    "    min(16, max(4, int(np.sqrt(dim)) + 2))\n",
    "    for dim in data.cat_dims\n",
    "]\n",
    "\n",
    "print(f\"Размерности эмбеддингов: {cat_emb_dims}\")\n",
    "print(f\"Суммарная размерность после конкатенации: {sum(cat_emb_dims) + data.num_numerical}\")\n",
    "\n",
    "# Инициализация модели\n",
    "graphsage = GraphSAGE(\n",
    "    cat_dims=data.cat_dims,\n",
    "    cat_emb_dims=cat_emb_dims,\n",
    "    num_numerical=data.num_numerical,\n",
    "    dim_h=64,\n",
    "    dim_out=2\n",
    ").to(device)\n",
    "\n",
    "# Обучение (функция train_with_monitoring без изменений)\n",
    "recall_sage, results_sage = train_with_monitoring(\n",
    "    graphsage, data, epochs=200, model_name=\"GraphSAGE (с эмбеддингами)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4308d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training GCN (с эмбеддингами) with resource monitoring\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Размерности эмбеддингов (адаптивная эвристика)\n",
    "cat_emb_dims = [min(16, max(4, int(np.sqrt(dim)) + 2)) for dim in data.cat_dims]\n",
    "\n",
    "# Инициализация модели\n",
    "gcn = GCN(\n",
    "    cat_dims=data.cat_dims,\n",
    "    cat_emb_dims=cat_emb_dims,\n",
    "    num_numerical=data.num_numerical,\n",
    "    dim_h=64,\n",
    "    dim_out=2\n",
    ").to(device)\n",
    "\n",
    "# Обучение\n",
    "recall_gcn, results_gcn = train_with_monitoring(\n",
    "    gcn, data, epochs=80, model_name=\"GCN (с эмбеддингами)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8db2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
