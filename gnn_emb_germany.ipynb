{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b5c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klema/miniconda3/envs/graphsage/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv, SAGEConv\n",
    "from torch_geometric.utils import to_undirected\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    NVML_AVAILABLE = True\n",
    "except:\n",
    "    NVML_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02be80d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Error connecting to server",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/urllib/request.py:1344\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/http/client.py:1338\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1337\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/http/client.py:1384\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1383\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/http/client.py:1333\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/http/client.py:1093\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1096\u001b[39m \n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/http/client.py:1037\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/http/client.py:1472\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1470\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mConnect to a host on a given (SSL) port.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1472\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/http/client.py:1003\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1002\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/socket.py:865\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ExceptionGroup(\u001b[33m\"\u001b[39m\u001b[33mcreate_connection failed\u001b[39m\u001b[33m\"\u001b[39m, exceptions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/socket.py:850\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    849\u001b[39m     sock.bind(source_address)\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: [Errno 110] Connection timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mURLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/site-packages/ucimlrepo/fetch.py:68\u001b[39m, in \u001b[36mfetch_ucirepo\u001b[39m\u001b[34m(name, id)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     response = \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_default_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcafile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcertifi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     data = json.load(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/urllib/request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/urllib/request.py:515\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    514\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/urllib/request.py:532\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    531\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/urllib/request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    491\u001b[39m func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/urllib/request.py:1392\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/urllib/request.py:1347\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[32m   1348\u001b[39m r = h.getresponse()\n",
      "\u001b[31mURLError\u001b[39m: <urlopen error [Errno 110] Connection timed out>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mucimlrepo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fetch_ucirepo\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m german_credit = \u001b[43mfetch_ucirepo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[32;43m144\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== FEATURES ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mТип:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(german_credit.data.features))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/graphsage/lib/python3.12/site-packages/ucimlrepo/fetch.py:71\u001b[39m, in \u001b[36mfetch_ucirepo\u001b[39m\u001b[34m(name, id)\u001b[39m\n\u001b[32m     69\u001b[39m     data = json.load(response)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib.error.URLError, urllib.error.HTTPError):\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mError connecting to server\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# verify that dataset exists \u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data[\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m] != \u001b[32m200\u001b[39m:\n",
      "\u001b[31mConnectionError\u001b[39m: Error connecting to server"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "german_credit = fetch_ucirepo(id=144)\n",
    "\n",
    "print(\"=== FEATURES ===\")\n",
    "print(\"Тип:\", type(german_credit.data.features))\n",
    "print(\"Форма:\", german_credit.data.features.shape if hasattr(german_credit.data.features, 'shape') else 'N/A')\n",
    "print(\"Колонки:\", german_credit.data.features.columns.tolist() if hasattr(german_credit.data.features, 'columns') else 'N/A')\n",
    "print(\"\\nПример признаков:\")\n",
    "print(german_credit.data.features.head(2))\n",
    "\n",
    "print(\"\\n=== TARGETS ===\")\n",
    "print(\"Тип:\", type(german_credit.data.targets))\n",
    "print(\"Форма:\", german_credit.data.targets.shape if hasattr(german_credit.data.targets, 'shape') else 'N/A')\n",
    "print(\"Колонки:\", german_credit.data.targets.columns.tolist() if hasattr(german_credit.data.targets, 'columns') else 'N/A')\n",
    "print(\"Уникальные значения:\", np.unique(german_credit.data.targets) if hasattr(german_credit.data.targets, 'values') else german_credit.data.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fe0992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Категориальные признаки (13): ['Attribute1', 'Attribute3', 'Attribute4', 'Attribute6', 'Attribute7', 'Attribute9', 'Attribute10', 'Attribute12', 'Attribute14', 'Attribute15', 'Attribute17', 'Attribute19', 'Attribute20']\n",
      "Числовые признаки (7): ['Attribute2', 'Attribute5', 'Attribute8', 'Attribute11', 'Attribute13', 'Attribute16', 'Attribute18']\n",
      "\n",
      "✅ Загружено 1000 узлов\n",
      "Числовые признаки: 7, Категориальные: 13\n",
      "Уникальные значения по кат. признакам: [4, 5, 10, 5, 5, 4, 3, 4, 3, 3, 4, 2, 2]\n",
      "Train/Val/Test: 600/200/200\n",
      "Распределение классов: [700 300] (0=good, 1=bad)\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from torch_geometric.utils import to_undirected\n",
    "import pandas as pd\n",
    "\n",
    "# Загрузка\n",
    "german_credit = fetch_ucirepo(id=144)\n",
    "X = german_credit.data.features  # DataFrame 1000 x 20: Attribute1..Attribute20\n",
    "y = german_credit.data.targets   # DataFrame 1000 x 1: колонка 'class'\n",
    "\n",
    "# Целевая переменная: 1 → 0 (good), 2 → 1 (bad)\n",
    "y = y['class'].values\n",
    "y = np.where(y == 1, 0, 1)  # good=0, bad=1\n",
    "\n",
    "# Согласно официальному описанию датасета (UCI):\n",
    "# Категориальные атрибуты (по индексу, начиная с 1)\n",
    "categorical_indices = [1, 3, 4, 6, 7, 9, 10, 12, 14, 15, 17, 19, 20]\n",
    "# Числовые атрибуты\n",
    "numerical_indices = [2, 5, 8, 11, 13, 16, 18]\n",
    "\n",
    "# Формируем списки имён колонок\n",
    "categorical_cols = [f'Attribute{i}' for i in categorical_indices]\n",
    "numerical_cols = [f'Attribute{i}' for i in numerical_indices]\n",
    "\n",
    "print(f\"Категориальные признаки ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Числовые признаки ({len(numerical_cols)}): {numerical_cols}\")\n",
    "\n",
    "# Обработка категориальных: преобразуем строки (типа 'A11') в индексы\n",
    "X_cat_encoded = np.zeros((len(X), len(categorical_cols)), dtype=np.int64)\n",
    "cat_dims = []\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    le = LabelEncoder()\n",
    "    # Все значения уже строки в формате 'A11', 'A12' и т.д.\n",
    "    X_cat_encoded[:, i] = le.fit_transform(X[col])\n",
    "    cat_dims.append(len(le.classes_))\n",
    "\n",
    "# Обработка числовых: стандартизация\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Построение графа (на основе всех признаков)\n",
    "X_combined = np.hstack([X_num_scaled, X_cat_encoded])\n",
    "k = 5\n",
    "adj_matrix = kneighbors_graph(X_combined, k, mode='connectivity', include_self=False)\n",
    "edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "edge_index = to_undirected(edge_index)\n",
    "\n",
    "# Разделение на train/val/test с сохранением баланса классов\n",
    "num_nodes = len(X)\n",
    "indices = np.arange(num_nodes)\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.4, random_state=42, stratify=y)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42, stratify=y[temp_idx])\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "# Создание объекта данных с разделёнными признаками\n",
    "data = Data(\n",
    "    x_num=torch.tensor(X_num_scaled, dtype=torch.float),\n",
    "    x_cat=torch.tensor(X_cat_encoded, dtype=torch.long),\n",
    "    edge_index=edge_index,\n",
    "    y=torch.tensor(y, dtype=torch.long),\n",
    "    train_mask=train_mask,\n",
    "    val_mask=val_mask,\n",
    "    test_mask=test_mask\n",
    ")\n",
    "\n",
    "# Метаданные для моделей\n",
    "data.num_numerical = X_num_scaled.shape[1]\n",
    "data.num_categorical = len(categorical_cols)\n",
    "data.cat_dims = cat_dims\n",
    "\n",
    "print(f\"\\n✅ Загружено {num_nodes} узлов\")\n",
    "print(f\"Числовые признаки: {data.num_numerical}, Категориальные: {data.num_categorical}\")\n",
    "print(f\"Уникальные значения по кат. признакам: {cat_dims}\")\n",
    "print(f\"Train/Val/Test: {train_mask.sum()}/{val_mask.sum()}/{test_mask.sum()}\")\n",
    "print(f\"Распределение классов: {np.bincount(y)} (0=good, 1=bad)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4a4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление весов перед обучением\n",
    "y_train = data.y[data.train_mask].cpu().numpy()\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Используем устройство целевой переменной (всегда присутствует)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(data.y.device)\n",
    "\n",
    "# Использование в лоссе\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b859d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred, true, probs=None):\n",
    "    pred = pred.cpu().numpy()\n",
    "    true = true.cpu().numpy()\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (pred == true).mean(),\n",
    "        'precision': precision_score(true, pred, zero_division=0),\n",
    "        'recall': recall_score(true, pred, zero_division=0),\n",
    "        'f1': f1_score(true, pred, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    if probs is not None:\n",
    "        probs = probs.cpu().numpy()\n",
    "        metrics['roc_auc'] = roc_auc_score(true, probs)\n",
    "        metrics['pr_auc'] = average_precision_score(true, probs)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "def test(model, data):\n",
    "    \"\"\"Тестирование с поддержкой обеих сигнатур: (x, edge_index) и (x_num, x_cat, edge_index)\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Автоматическое определение сигнатуры модели по количеству параметров в forward\n",
    "        if hasattr(data, 'x_num') and hasattr(data, 'x_cat'):\n",
    "            # Новая сигнатура с эмбеддингами\n",
    "            _, out = model(data.x_num, data.x_cat, data.edge_index)\n",
    "        else:\n",
    "            # Старая сигнатура с объединёнными признаками\n",
    "            _, out = model(data.x, data.edge_index)\n",
    "        \n",
    "        pred = out.argmax(dim=1)[data.test_mask]\n",
    "        probs = torch.exp(out)[:, 1][data.test_mask]  # Вероятность класса 1 (дефолт)\n",
    "        true = data.y[data.test_mask]\n",
    "        \n",
    "        return compute_metrics(pred, true, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b724ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeatureEncoder(torch.nn.Module):\n",
    "    def __init__(self, cat_dims, cat_emb_dims, num_numerical, output_dim):\n",
    "        super().__init__()\n",
    "        assert len(cat_dims) == len(cat_emb_dims)\n",
    "        \n",
    "        self.embeddings = torch.nn.ModuleList([\n",
    "            torch.nn.Embedding(num_categories, emb_dim)\n",
    "            for num_categories, emb_dim in zip(cat_dims, cat_emb_dims)\n",
    "        ])\n",
    "        \n",
    "        total_input_dim = sum(cat_emb_dims) + num_numerical\n",
    "        self.projector = torch.nn.Sequential(\n",
    "            torch.nn.Linear(total_input_dim, output_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_num, x_cat):\n",
    "        embeddings = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        x = torch.cat([x_num] + embeddings, dim=1)\n",
    "        return self.projector(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "557aec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, cat_dims, cat_emb_dims, num_numerical, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.feature_encoder = FeatureEncoder(\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dims=cat_emb_dims,\n",
    "            num_numerical=num_numerical,\n",
    "            output_dim=dim_h\n",
    "        )\n",
    "        self.sage1 = SAGEConv(dim_h, dim_h)\n",
    "        self.sage2 = SAGEConv(dim_h, dim_out)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    def forward(self, x_num, x_cat, edge_index):\n",
    "        x = self.feature_encoder(x_num, x_cat)\n",
    "        h = self.sage1(x, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.sage2(h, edge_index)\n",
    "        return h, F.log_softmax(h, dim=1)\n",
    "\n",
    "    def fit(self, data, epochs, patience=20):\n",
    "        # Взвешенный лосс для дисбаланса классов\n",
    "        y_train = data.y[data.train_mask].cpu().numpy()\n",
    "        weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        weights = torch.tensor(weights, dtype=torch.float).to(data.x_num.device)\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        # Ранняя остановка\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            _, out = self(data.x_num, data.x_cat, data.edge_index)  # ← ключевое изменение\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Метрики на трейне\n",
    "                train_pred = out[data.train_mask].argmax(dim=1)\n",
    "                train_metrics = compute_metrics(train_pred, data.y[data.train_mask])\n",
    "                \n",
    "                # Метрики на валидации\n",
    "                val_pred = out[data.val_mask].argmax(dim=1)\n",
    "                val_probs = torch.exp(out)[:, 1][data.val_mask]\n",
    "                val_metrics = compute_metrics(val_pred, data.y[data.val_mask], probs=val_probs)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            # Ранняя остановка\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_state = {k: v.cpu() for k, v in self.state_dict().items()}\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience and epoch > 30:  # минимум 30 эпох обучения\n",
    "                    print(f\"\\n⏹️  Early stopping at epoch {epoch} (best val loss: {best_val_loss:.3f})\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    break\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch:>3} | '\n",
    "                    f'TL: {loss:.3f} | TF1: {train_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VL: {val_loss:.3f} | VF1: {val_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VRec: {val_metrics[\"recall\"]:.3f} | VPR: {val_metrics[\"pr_auc\"]:.3f}')\n",
    "                \n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, cat_dims, cat_emb_dims, num_numerical, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        # Энкодер признаков: эмбеддинги + проекция\n",
    "        self.feature_encoder = FeatureEncoder(\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dims=cat_emb_dims,\n",
    "            num_numerical=num_numerical,\n",
    "            output_dim=dim_h  # проецируем сразу в размерность первого GCN-слоя\n",
    "        )\n",
    "        \n",
    "        # Графовые слои\n",
    "        self.gcn1 = GCNConv(dim_h, dim_h)\n",
    "        self.gcn2 = GCNConv(dim_h, dim_out)\n",
    "        \n",
    "        # Оптимизатор с дифференцированной регуляризацией\n",
    "        # (сильнее регуляризуем эмбеддинги для борьбы с переобучением)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.feature_encoder.embeddings.parameters(), 'weight_decay': 1e-3},\n",
    "            {'params': self.feature_encoder.projector.parameters(), 'weight_decay': 5e-4},\n",
    "            {'params': self.gcn1.parameters(), 'weight_decay': 5e-4},\n",
    "            {'params': self.gcn2.parameters(), 'weight_decay': 5e-4},\n",
    "        ], lr=0.02)\n",
    "\n",
    "    def forward(self, x_num, x_cat, edge_index):\n",
    "        # Шаг 1: кодирование признаков через эмбеддинги\n",
    "        x = self.feature_encoder(x_num, x_cat)\n",
    "        \n",
    "        # Шаг 2: графовая обработка (сохраняем архитектуру оригинальной GCN)\n",
    "        h = F.dropout(x, p=0.5, training=self.training)\n",
    "        h = self.gcn1(h, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.gcn2(h, edge_index)\n",
    "        return h, F.log_softmax(h, dim=1)\n",
    "\n",
    "    def fit(self, data, epochs, patience=20):\n",
    "        # Взвешенный лосс для дисбаланса классов\n",
    "        y_train = data.y[data.train_mask].cpu().numpy()\n",
    "        weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        weights = torch.tensor(weights, dtype=torch.float).to(data.x_num.device)\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        # Ранняя остановка\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "\n",
    "        self.train()\n",
    "        for epoch in range(epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            _, out = self(data.x_num, data.x_cat, data.edge_index)  # ← ключевое изменение\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Метрики на трейне\n",
    "                train_pred = out[data.train_mask].argmax(dim=1)\n",
    "                train_metrics = compute_metrics(train_pred, data.y[data.train_mask])\n",
    "                \n",
    "                # Метрики на валидации\n",
    "                val_pred = out[data.val_mask].argmax(dim=1)\n",
    "                val_probs = torch.exp(out)[:, 1][data.val_mask]\n",
    "                val_metrics = compute_metrics(val_pred, data.y[data.val_mask], probs=val_probs)\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "            # Ранняя остановка\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_state = {k: v.cpu() for k, v in self.state_dict().items()}\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience and epoch > 30:  # минимум 30 эпох обучения\n",
    "                    print(f\"\\n⏹️  Early stopping at epoch {epoch} (best val loss: {best_val_loss:.3f})\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    break\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch:>3} | '\n",
    "                    f'TL: {loss:.3f} | TF1: {train_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VL: {val_loss:.3f} | VF1: {val_metrics[\"f1\"]:.3f} | '\n",
    "                    f'VRec: {val_metrics[\"recall\"]:.3f} | VPR: {val_metrics[\"pr_auc\"]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "980bc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "\n",
    "def monitor_resources():\n",
    "    stats = {}\n",
    "    # CPU & RAM\n",
    "    stats['ram_mb'] = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    stats['cpu_percent'] = psutil.cpu_percent()\n",
    "\n",
    "    # GPU\n",
    "    if device.type == 'cuda' and NVML_AVAILABLE:\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "        stats['gpu_mem_mb'] = mem_info.used / (1024 ** 2)\n",
    "        stats['gpu_util'] = util.gpu\n",
    "    else:\n",
    "        stats['gpu_mem_mb'] = None\n",
    "        stats['gpu_util'] = None\n",
    "    return stats\n",
    "\n",
    "def train_with_monitoring(model, data, epochs, model_name):\n",
    "    print(f\"\\n{'='*50}\\nTraining {model_name} with resource monitoring\\n{'='*50}\")\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.fit(data, epochs)\n",
    "\n",
    "    final_ram = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    max_gpu_mem = None\n",
    "    if device.type == 'cuda':\n",
    "        max_gpu_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    test_metrics = test(model, data)\n",
    "\n",
    "    results = {\n",
    "        'test_metrics': test_metrics,\n",
    "        'training_time_sec': duration,\n",
    "        'final_ram_mb': final_ram,\n",
    "        'max_gpu_mem_mb': max_gpu_mem,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{model_name} finished\")\n",
    "    print(f\"Test Recall: {test_metrics.get('recall', 0):.3f} | \"\n",
    "          f\"F1: {test_metrics.get('f1', 0):.3f} | \"\n",
    "          f\"PR-AUC: {test_metrics.get('pr_auc', 0):.3f}\")\n",
    "    print(f\"Training Time: {duration:.1f} sec\")\n",
    "    if max_gpu_mem:\n",
    "        print(f\"Peak GPU Memory: {max_gpu_mem:.1f} MB\")\n",
    "    print(f\"Final RAM Usage: {final_ram:.1f} MB\")\n",
    "\n",
    "    return test_metrics.get('recall', 0), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e61273e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерности эмбеддингов: [4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Суммарная размерность после конкатенации: 60\n",
      "\n",
      "==================================================\n",
      "Training GraphSAGE (с эмбеддингами) with resource monitoring\n",
      "==================================================\n",
      "Epoch   0 | TL: 0.698 | TF1: 0.327 | VL: 0.709 | VF1: 0.315 | VRec: 0.333 | VPR: 0.332\n",
      "Epoch  10 | TL: 0.584 | TF1: 0.615 | VL: 0.578 | VF1: 0.595 | VRec: 0.733 | VPR: 0.574\n",
      "Epoch  20 | TL: 0.526 | TF1: 0.640 | VL: 0.553 | VF1: 0.605 | VRec: 0.767 | VPR: 0.660\n",
      "Epoch  30 | TL: 0.417 | TF1: 0.714 | VL: 0.570 | VF1: 0.595 | VRec: 0.733 | VPR: 0.556\n",
      "\n",
      "⏹️  Early stopping at epoch 31 (best val loss: 0.532)\n",
      "\n",
      "GraphSAGE (с эмбеддингами) finished\n",
      "Test Recall: 0.733 | F1: 0.607 | PR-AUC: 0.617\n",
      "Training Time: 0.4 sec\n",
      "Final RAM Usage: 5729.7 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "\n",
    "# Адаптивные размерности эмбеддингов: √(уникальных значений) + небольшой запас\n",
    "cat_emb_dims = [\n",
    "    min(16, max(4, int(np.sqrt(dim)) + 2))\n",
    "    for dim in data.cat_dims\n",
    "]\n",
    "\n",
    "print(f\"Размерности эмбеддингов: {cat_emb_dims}\")\n",
    "print(f\"Суммарная размерность после конкатенации: {sum(cat_emb_dims) + data.num_numerical}\")\n",
    "\n",
    "# Инициализация модели\n",
    "graphsage = GraphSAGE(\n",
    "    cat_dims=data.cat_dims,\n",
    "    cat_emb_dims=cat_emb_dims,\n",
    "    num_numerical=data.num_numerical,\n",
    "    dim_h=64,\n",
    "    dim_out=2\n",
    ").to(device)\n",
    "\n",
    "# Обучение (функция train_with_monitoring без изменений)\n",
    "recall_sage, results_sage = train_with_monitoring(\n",
    "    graphsage, data, epochs=100, model_name=\"GraphSAGE (с эмбеддингами)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3dcaaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерности эмбеддингов: [4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Суммарная размерность после конкатенации: 60\n",
      "\n",
      "==================================================\n",
      "Training GCN (с эмбеддингами) with resource monitoring\n",
      "==================================================\n",
      "Epoch   0 | TL: 0.718 | TF1: 0.463 | VL: 0.718 | VF1: 0.436 | VRec: 0.850 | VPR: 0.300\n",
      "Epoch  10 | TL: 0.651 | TF1: 0.529 | VL: 0.647 | VF1: 0.533 | VRec: 0.867 | VPR: 0.528\n",
      "Epoch  20 | TL: 0.590 | TF1: 0.574 | VL: 0.558 | VF1: 0.577 | VRec: 0.683 | VPR: 0.642\n",
      "Epoch  30 | TL: 0.553 | TF1: 0.611 | VL: 0.534 | VF1: 0.607 | VRec: 0.733 | VPR: 0.684\n",
      "Epoch  40 | TL: 0.562 | TF1: 0.605 | VL: 0.577 | VF1: 0.582 | VRec: 0.683 | VPR: 0.625\n",
      "Epoch  50 | TL: 0.536 | TF1: 0.616 | VL: 0.548 | VF1: 0.609 | VRec: 0.817 | VPR: 0.652\n",
      "\n",
      "⏹️  Early stopping at epoch 53 (best val loss: 0.522)\n",
      "\n",
      "GCN (с эмбеддингами) finished\n",
      "Test Recall: 0.650 | F1: 0.582 | PR-AUC: 0.578\n",
      "Training Time: 0.9 sec\n",
      "Final RAM Usage: 5731.0 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "\n",
    "# Адаптивные размерности эмбеддингов: √(уникальных значений) + небольшой запас\n",
    "cat_emb_dims = [\n",
    "    min(16, max(4, int(np.sqrt(dim)) + 2))\n",
    "    for dim in data.cat_dims\n",
    "]\n",
    "\n",
    "print(f\"Размерности эмбеддингов: {cat_emb_dims}\")\n",
    "print(f\"Суммарная размерность после конкатенации: {sum(cat_emb_dims) + data.num_numerical}\")\n",
    "\n",
    "# Инициализация модели\n",
    "graphsage = GCN(\n",
    "    cat_dims=data.cat_dims,\n",
    "    cat_emb_dims=cat_emb_dims,\n",
    "    num_numerical=data.num_numerical,\n",
    "    dim_h=64,\n",
    "    dim_out=2\n",
    ").to(device)\n",
    "\n",
    "# Обучение (функция train_with_monitoring без изменений)\n",
    "recall_sage, results_sage = train_with_monitoring(\n",
    "    graphsage, data, epochs=100, model_name=\"GCN (с эмбеддингами)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ace0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
